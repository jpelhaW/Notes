### mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
---
- [Zotero Select Link](zotero://select/groups/2480461/items/H9MRQ4N8)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/H9MRQ4N8)
- Authors: [[Linting Xue]] [[Noah Constant]] [[Adam Roberts]] [[Colin Raffel]]
- Topics: [[nlp_lm]] [[nlp_transformers]] [[ann_architecture]]
- Venue: #NAACL 
- Year: #2021
---
### Major Contributions
- we introduce [[_2021_mT5]], a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages
- 
---
### Secondary Contribution
- Ablation well justified
	- We run six ablations, modifying various settings, using our Large model as a baseline: (i) increase dropout 0.1 in hopes of mitigating overfitting on low-resource languages, (ii) decrease sequence length to 512 (as was used in T5), (iii) increase the average noise span length in the pre-training objective to 10  since we observe fewer characters per token than T5, (iv) adjust the language sampling exponent alpha to {0.2, 0.7} as used in MMNMT (Ariazhagan  t al., 2019) and mBERT (Devlin, 2018), espectively, (v) turn off the “line length filter” in the mC4 data pipeline, and (vi) supplement mC4 data ith Wikipedia from 103 languages
- They Explore several pitfall from their model in section 5 (Zero-Shot Generation)
	- Illegal predictions
		- normalization
		- grammatical adjustment
		- accidental translation
	- Preventing Accidental Translation
	- 
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- To train [[_2021_mT5]], we introduce a multilingual variant of the C4 dataset called mC4. mC4 comprises natural text in 101 languages drawn from the public Common Crawl web scrape.
- we base mT5 on the “T5.1.1” recipe,5 which improves upon T5 by using GeGLU nonlinearities (Shazeer, 2020),
- mT5 use SentencePiece (Kudo and Richardson, 2018; Kudo, 2018) models trained with the language sampling rates used during pre-training
- Following the original T5 recipe, we consider five model sizes: Small, Base, Large, XL, XXL
- Note that unlike our model, InfoXLM (Chi et al., 2020) and VECO (Luo et al., 2020) benefit from parallel training data, while X-STILTs (Phang et al., 2020) leverages labeled data from tasks similar to the target task.
- Since mT5 is a generative model, it can output arbitrary text predictions in a free form fashion. This is in contrast to “encoder-only” models like mBERT and XLM(-R) that make a prediction by either extracting it from the input or producing a class label.
- 
---
