---
- Link: <if exists>
- Auhtor: [[Mike Lewis]] [[Luke Zettlemoyer]] [[Omer Levy]]
- Topics: [[nlp_transformers]]	]] [[#topic2]]
- Venue: #arXiv
- Year: #2019
---
### Techniques
- 
-
---
### Goal
---
### Major Contributions
- They corrupt the input text with a noisy functions and try to reconstruct the original text using a [[Seq2Seq]] model
- Combines [[bi-directional]] and [[auto-regressive]] [[Transformers]]
---
### Secondary Contribution
- Best approaches for corrupting words: random shuffling the order os the sentences and #in-filling scheme
- Works good for [[text-generation]]
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- The input text is encoded
![[2019_BART_Schematics.png]]

---
