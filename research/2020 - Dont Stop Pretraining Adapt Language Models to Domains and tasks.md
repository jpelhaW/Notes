---
- Auhtor: [[Suchin Gururangan]] [[Noah Smith]] [[Iz Beltagy]]
- Topics: [[nlp_lm]] [[nlp_task]] [[nlp_lm]]
- Venue: #ACL
- Year: #2020
---
### Techniques
---
### Goal
They show a second [[Pretraining]] in domain domain-adaptative pretraining (DAPT) leads to performance gains, under both high and low resource settings.

---
### Major Contributions
- Do large pretrained models work universally or is still helpful to build separate pretrained models for specific domains?
- Highlight of how task-adaptative pretraining [[TAPT]] and [[DAPT]]
---
### Secondary Contribution
- [[TAPT]] is even better when unlabeled data from the task distribution 
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
---

