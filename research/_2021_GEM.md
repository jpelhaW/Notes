### The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics
---
- [Zotero Select Link](zotero://select/groups/2480461/items/LKQMS2E4)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/LKQMS2E4)
- Authors: [[Sebastian Gehrmann]] [[Jiawei Zhou]] 
- Topics: [[nlp_text_generation]] [[nlp_ben]]
- Venue: #arXiv 
- Year: #2021
---
### Major Contributions
- [GEM-link](https://gem-benchmark.com/.)
- GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics.
- We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges.
- To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress.
---
### Secondary Contribution
- GEM features 18 languages across all tasks and two of the datasets do not include English at all.
- Their dataset and models are all follow the new standards Datasheets for datasets and Model cards [[_2019_ModelCards]] and [[_2020_DatasheetsDatasets]].
- They present an interesting methodology to survey the inclusion/exclusion of datasets in Appendices A
	- Appendices B and C show the complete datasets considered and details about them
- We apply multiple strategies to create the special test sets, in particular (I) alteration of the existing test sets (e.g., the introduction of distractors), (II) breaking down of the existing sets into subsets with certain properties (e.g., subsets with different complexity), and (III) the compilation of new test sets (e.g., out-of-vocabulary inputs).
- Tables 3 and 4 in the paper shows a good summary of several baselines for  the (sub)datasets in [[_2021_GEM]].
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent.
- As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Otherwise, results are challenging to interpret and compare to each other.
- many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages.
- We produce data cards (Bender and Friedman,2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-specific template.
- Our main baselines are [[_2019_T5]] with 60M parameters (Raffel et al., 2020) and [[_2019_BART]] with 139M parameters (Lewis et al., 2020a).
- ![[2021_GEM_dataset_details.png]]
---
