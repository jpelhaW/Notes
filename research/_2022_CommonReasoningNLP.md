### Introductory Tutorial: Commonsense Reasoning for Natural Language Processing
---
- [Zotero Select Link](zotero://select/groups/2480461/items/525I8LZ4)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/525I8LZ4)
- Authors: [[Maarten Sap]] [[Dan Roth]]
- Topics: [[nlp_knowlegde_representation]] [[nlp_transformers]] [[tutorial]]
- Venue: #ACL 
- Year: #2020
---
### Major Contributions
- (1) outline the various types of commonsense (e.g., physical, social)
- (2) discuss techniques to gather and represent commonsense knowledge, while highlighting the challenges specific to this type of knowledge (e.g., reporting bias). 
- (3) discuss the types of commonsense knowledge captured by modern NLP systems (e.g., large pretrained language models)
- (4) review ways to incorporate commonsense knowledge into downstream task models
- (5) present various benchmarks used to measure systemsâ€™ commonsense reasoning abilities.
---
### Secondary Contribution
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- More recently, rather than providing relevant commonsense as an additional input to neural networks, researchers have looked into indirectly encoding commonsense knowledge into the parameters of neural networks through pretraining on commonsense knowledge bases (Zhong et al.,2018) or explanations (Rajani et al., 2019), or by using multi-task objectives with commonsense relation prediction (Xia et al., 2019).
---
