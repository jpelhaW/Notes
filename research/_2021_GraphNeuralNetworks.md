### Graph Neural Networks for Natural Language Processing: A Survey
---
- [Zotero Select Link](zotero://select/groups/2480461/items/697X4KAZ)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/697X4KAZ)
- Authors: [[Lingfei Wu]] [[Bo Long]]
- Topics: [[nlp_survey]] [[nlp_graph]]
- Venue: #arXiv 
- Year: #2021
---
### Major Contributions
- a comprehensive overview on Graph Neural Networks (GNNs) for Natural Language processing. We propose a new taxonomy of GNNs for NLP, which systematically organizes existing research of GNNs for NLP along three axes: graph construction, graph representation learning, and graph based encoder-decoder models. We further introduce a large number of NLP applications that are exploiting the power of GNNs and summarize the corresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, we discuss various outstanding challenges for making the full use of GNNs for NLP
---
### Secondary Contribution
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- Really long and complete survey (127pp)
- Despite the successes these existing research has achieved, deep learning on graphs for NLP still encounters many challenges:
	- Automatically transforming original text sequence data into highly graph-structured data.
	- Properly determining graph representation learning techniques
	- Effectively modeling complex data
- GNNs for NLP along three axes: 
	- graph construction
	- graph representation learning
	- graph based encoder-decoder models.
- Table 2 - lists several related work in static and dynamics graph construction
- Table 3 - lists relevant work  in NLP using GNN, including the task, application, and evaluation
- ![[2021_GraphNN_taxonomy.png]]
---
