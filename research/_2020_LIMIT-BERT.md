### LIMIT-BERT : Linguistics Informed Multi-Task BERT
---
- [Zotero Select Link](zotero://select/groups/2480461/items/6MMBEKT9)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/6MMBEKT9)
- Authors: [[Junru Zhou]] [[Shuailiang Zhang]]
- Topics: [[nlp_transformers]] [[nlp_nlu]] [[nlp_multi_task]] [[_2019_BERT]]
- Venue: #arXiv #EMNLP 
- Year: #2020
---
### Major Contributions
- A Multi-Task BERT inspired model that includes five key linguistics tasks
	- Part-Of-Speech (POS) tags, constituent and dependency syntactic parsing, span and depen dency semantic role labeling (SRL).
---
### Secondary Contribution
- Joint learning is a better way to let pre-training language models help linguistic inspired NLP problems in a bidirectional mode (instead of unidirectional)
- Naturally empoweredby linguistic clues from joint learning, pre-trained language models will be more powerful for enhancing downstream tasks.
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- most existing language models are trained on large amounts of data without explicitly ecxploiting lingustic knowledge
- they use [[_2020_ELECTRA]] training method with the generator-discriminator architecture
- incorporating linguistic knowledge into pre-trained language model by multi-task and semi-supervised learning can significantly enhance downstream tasks.
---
