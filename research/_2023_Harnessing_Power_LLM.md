### Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond
---
- [Zotero Select Link](zotero://select/groups/2480461/items/X6QWSPAX)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/X6QWSPAX)
- Authors: [[JingFeng Yang]]  [[XIA HU]] 
- Topics: [[nlp_explain]] [[nlp_llm]] [[nlp_train]]
- Venue: #arxiv
- Year: #2023

---
### Major Contributions
- We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks
- we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks
- This work summarizes the following main practical guides for using LLMs:
	- Natural language understanding. Employ the exceptional generalization ability of LLMs when facing out-ofdistribution data or with very few training data.
	- Natural language generation. Utilize LLMs’ capabilities to create coherent, contextually relevant, and highquality text for various applications.
	- Knowledge-intensive tasks. Leverage the extensive knowledge stored in LLMs for tasks requiring domainspecific expertise or general world knowledge.
	- Reasoning ability. Understand and harness the reasoning capabilities of LLMs to improve decision-making and problem-solving in various contexts.
---
### Secondary Contribution
- [GitHub link](https://github.com/Mooler0410/LLMsPracticalGuide)
- In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models. For sentiment analysis, such as on IMDB [69] and SST [94], fine-tuned models and LLMs perform equally well. For toxicity detection, which is another iconic text classification task, the gap is much larger. All LLMs cannot perform well on this task, and on CivilComments [13] even the best one is only better than random guessing [59]. On the other hand, most popular fine-tuned models can obtain much better performance [33].
- Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [16].
---
### Limitations/Future Work
- 
---
### Notes (Try to use backlinks)
- the definitions of them are proposed as: 
	- LLMs are huge language models pretrained on large amounts of datasets without tuning on data for specific tasks;
	- fine-tuned models are typically smaller language models which are also pretrained and then further tuned on a smaller, task-specific dataset to optimize their performance on that task
- Meta stands out as one of the most generous commercial companies, as all the LLMs developed by Meta are open-sourced
- Encoder-Decoder or Encoder-only: predict masked words in a sentence while considering the surrounding context. This training paradigm is known as the Masked Language Model. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used
- Decoder-only: scaling up language models significantly improves the few-shot, even zero-shot performance [16]. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words.
- **Remarks**
	- LLMs generalize better than fine-tuned models in downstream tasks facing out-of-distribution data, such as adversarial examples and domain shifts.
	- LLMs are preferable to fine-tuned models when working with limited annotated data, and both can be reasonable choices when abundant annotated data is available, depending on specific task requirements.
	- It’s advisable to choose models pre-trained on fields of data that are similar to downstream tasks.
	- LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.
	- LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.
- The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.
- Considering Finetuning data:
	- **Zero annotated data:** In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach.
	- **Few annotated data:** In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA fine-tuned open-domain models
	- **Abundant annotated data:** With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy [99]
	- Natural Language Generation broadly encompasses two major categories of tasks, with the goal of creating coherent, meaningful, and contextually appropriate sequences of symbols. The first type focuses on converting input texts into new symbol sequences, as exemplified by tasks like paragraph summarization and machine translation. The second type, "open-ended" generation, aims to generate text or symbols from scratch to accurately match input descriptions such as crafting emails, composing news articles, creating fictional stories and writing code.
- ![[2023_Harnessing_Power_LLM_tree.png]]
- 
- ![[2023_Harnessing_Power_LLM_summarymodels.png]]
- ![[2023_Harnessing_Power_LLM_decision.png]]
---
