### CM3: A Causal Masked Multimodal Model of the Internet
---
- [Zotero Select Link](zotero://select/groups/2480461/items/AZCMMXJU)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/AZCMMXJU)
- Authors: [[Armen Aghajanyan]] [[Luke Zettlemoyer]] 
- Topics: [[facebook]] [[nlp_lm]] [[nlp_train]] [[nlp_cv]]
- Venue: #arXiv #facebook
- Year: #2022
---
### Major Contributions
- Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. 
	- The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. 
	- We train causally masked language image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking).
---
### Secondary Contribution
---
### Limitations/Future Work
- Their casual masked langugage modeling is not very detailed -> it produces good results in a couple NLP tasks
	- They taske the mask span and put at the end of the sequence. - Why? No good explanation.
---
### Notes (Try to use backlinks)
- We demonstrate consistently strong transfer from to a range of uni-modal and multi CM3 modal tasks at differing supervision levels, including stating state-of-the-art on entity disambiguation and zero-shot summarization.
- We introduce a novel objective that combines the benefit of per-token generation with optional bi-directionality specifically tailored to prompting.
- ![[2022_CM3_example_arch.png]]
- the causally masked objective can do causal language modeling while optionally allowing for bidirectionality when needed
- [[CM3|Causally-Masked Multimodal Modeling]] extends this work by modeling full document structure including images and hypertext links. Furthermore, we move away from the BART-like objective of Aghajanyan et al. (2021) to use our new causally masked objective with decoder-only models.
---
