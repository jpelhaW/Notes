ğŸ”¹ Â The First Law of ComplexodynamicsÂ Â   
ğŸ”¹ The Unreasonable Effectiveness of Recurrent Neural NetworksÂ Â   
ğŸ”¹ Understanding LSTM NetworksÂ Â   
ğŸ”¹ Recurrent Neural Network RegularizationÂ Â   
ğŸ”¹ Keeping Neural Networks Simple by Minimizing the Description Length of the WeightsÂ Â   
ğŸ”¹ Pointer NetworksÂ Â   
ğŸ”¹ ImageNet Classification with Deep Convolutional Neural NetworksÂ Â   
ğŸ”¹ Order Matters: Sequence to Sequence for SetsÂ Â   
ğŸ”¹ GPipe: Easy Scaling with Micro-Batch Pipeline ParallelismÂ Â   
ğŸ”¹ Deep Residual Learning for Image RecognitionÂ Â   
ğŸ”¹ Multi-Scale Context Aggregation by Dilated ConvolutionsÂ Â   
ğŸ”¹ Neural Message Passing for Quantum ChemistryÂ Â   
ğŸ”¹ Attention is All You NeedÂ Â   
ğŸ”¹ Neural Machine Translation by Jointly Learning to Align and TranslateÂ Â   
ğŸ”¹ Identity Mappings in Deep Residual NetworksÂ Â   
ğŸ”¹ A Simple Neural Network Module for Relational ReasoningÂ Â   
ğŸ”¹ Variational Lossy AutoencoderÂ Â   
ğŸ”¹ Relational Recurrent Neural NetworksÂ Â   
ğŸ”¹ Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee AutomatonÂ Â   
ğŸ”¹ Neural Turing MachinesÂ Â   
ğŸ”¹ Deep Speech 2Â   
ğŸ”¹ Scaling Laws for Neural Language ModelsÂ Â   
ğŸ”¹ A Tutorial Introduction to the Minimum Description Length PrincipleÂ Â   
ğŸ”¹ Machine Super IntelligenceÂ Â   
ğŸ”¹ Kolmogorov Complexity and Algorithmic Randomness