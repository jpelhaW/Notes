### ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding
---
- [Zotero Select Link](zotero://select/groups/2480461/items/CLZ34N6U)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/CLZ34N6U)
- Authors: [[Yu Sun]] [[Haifeng Wang]]
- Topics: [[nlp_transformers]] [[nlp_multi_task]]
- Venue: #AAAI
- Year: #2020
---
### Major Contributions
- Proposes the continual learning to train their model with several tasks in sequence, so it can keep track of previous learned tasks when new ones are incorporated
- Two main concern [[_2020_ERNIE2.0]] focus with Continual Multi-task Learning: 
	- How to train tasks continuously without forgetting the previously learned ones (accumulative training)
	- How to pre-train multiple tasks efficiently
- Continual Multi-Task learning presents the best performance on downstream tasks compared with "continual pre-training" and "multi-task learning"
---
### Secondary Contribution
- Three pre-training tasks (self-supervised or weak-supervised):
	- Word-aware:  knowledge masking ([[_2019_ERNIE]]), token-document relation, capital prediction
	- Structure-aware: sentence reordering, sentence distance
	- Semantic-aware: discourse relation, IR relevance
- Continual learning is outperformed by continual multi-task and multi-task learning. When on task is learned independently from another the model tend to forget the prior to learn a new one.
---
### Limitations/Future Work
- How exactly the pre-training data-tasks were parsed?
- Focus on more challenging pre-training tasks?
---
### Notes (Try to use backlinks)
- [[_2020_ERNIE2.0]] framework
- ![[2020_ERNIE2.0_framework.png]]
- Continual pre-training contains two steps:
	- unsupervised pre-training tasks with big data and prior knowledge involved
	- incrementally update of ERNIE via continual multi-task learning
- They use the learned parameters from one task to initialize a new one
- ![[2020_ERNIE2.0_continual_training.png]]
- Traditional continual learning trains the model one task at a time. [[_2020_ERNIE2.0]] does this differently by sharing parameters between tasks.
	- ![[2020_ERNIE2.0_multitask_learning.png]]
- Their Table 7 shows the result of different continual pre-training tasks methodologies

---
