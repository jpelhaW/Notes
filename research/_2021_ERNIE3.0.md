### ERNIE 3.0: Large-Scale Knowledge Enhanced Pre-Training for Language Understanding and Generation
---
- [Zotero Select Link](zotero://select/groups/2480461/items/YK2L8PFI)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/YK2L8PFI)
- Authors: [[Yu Sun]] [[Haifeng Wang]]
- Topics: [[nlp_transformers]] [[nlp_multi_task]]
- Venue: #arXiv
- Year: #2021
---
### Major Contributions
- [[_2021_ERNIE3.0]] combines autoregressive and autoencoder
- A framework to pre-train model on massive unsupervised corpus including plain texts and knowledge graph.
- [[_2021_ERNIE3.0]] designs a new Continual Multi-Paradigms Unified Pre-training Framework - inspired/based on [[_2020_ERNIE2.0]]
---
### Secondary Contribution
- [[_2021_ERNIE3.0]] SOTA in 54 benchmarks - 4TB of corpus +10Bi parameters
- 
---
### Limitations/Future Work
- 
---
### Notes (Try to use backlinks)
- Problems with (most) large-scale language models:
	- Models are trained on plain text - no external linguistic knowlegde
	- Models are trained on auto-regressive layout - problems for fine-tuning in solving traditional tasks
- The 175-billion-parameter GPT-3 is trained on a corpus with 570GB filtered texts from Common Crawl. Such raw texts lack explicit representation of knowledge such as linguistic knowledge and world knowledge
- Different task paradigms of natural language processing depend on identical underlying abstract features consistently, such as lexical information and syntactic information, but the requirements of top-level concrete features are incompatible, in which the natural language understanding tasks have the disposition to learn the semantic coherence while natural language generation tasks expect further contextual information.
- We refer to the backbone shared network and task-specific networks as the Universal Representation Module and Task-specific Representation Modules in ERNIE 3.0
- Universal Representation module: Uses Transformer-XL as backbone
- Task-specific Representation module: Uses Transformer-XL
	- task-specific networks with base model size
	- stronger ability to capture semantic information
	- smaller model size of a task-specific network
- Progressive Learning to Speed up Convergence made things really fast for the training
- ![[2021_ERNIE3_arch.png]]

### Pre-training Tasks
- **Word-aware pre-trained tasks**  
	- **Knowledge Masked Language Modeling** (from [[_2019_ERNIE]]) introduced phrase masking and named entity masking that predict the whole masked phrases and named entities
	- **Document language modeling:** ERNIE 3.0 opt for traditional language model as the pre-training task to abate the network complexity and heighten the effectiveness of unifie pre-training. In addition, to enable the NLG network of ERNIE 3.0 to model longer text, we introduce the Enhanced Recurrence Memory Mechanism proposed in ERNIE-Doc
- **Structure-aware Pre-training Tasks**
	- **Sentence Reordering**  aims to train the model to learn the relationship between sentences by reorganizing permuted segments. (from [[_2020_ERNIE2.0]])
	- **Sentence Distance** Sentence distance task, an extension of traditional next sentence prediction (NSP) task, is widely used in various pre-trained models to enhance their ability to learn the sentence-level information
- **Knowledge-aware Pre-training Tasks**
	- **Universal Knowledge-Text Prediction** - extension of knowledge masked language modeling. The universal knowledge-text prediction taskrequires both unstructured texts and knowledge graphs
		- Given a pair of triple from knowledge graph and the corresponding sentence from encyclopedia, we randomly mask the relation in triple or words in a sentence. To predict the relation in the triple, the model needs to detect mentions of head entity and tail entity and determine semantic relationship that holds between them in the corresponding sentence.
		- The essence of this process is similar to the distant supervision algorithm [40] in relation extraction tasks. The distant supervision algorithm assume that if two entities participate in a relation, any sentence that contain those two entities might express that relation. 
		- Meanwhile, to predict words in the corresponding sentence, the model not only considers the dependency information in the sentence, but also logical relationship in the triple.
		- ![[2021_ERNIE3_UniversalKnowledgeTextPrediction.png]]

---
