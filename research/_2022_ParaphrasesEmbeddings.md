### Using Paraphrases to Study Properties of Contextual Embeddings
---
- [Zotero Select Link](zotero://select/groups/2480461/items/HEEV8SK6)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/HEEV8SK6)
- Authors: [[Laura Burdick]] [[Rada Mihalcea]] 
- Topics: [[nlp_embeddings]] [[nlp_paraphrase]]
- Venue: #NAACL 
- Year: #2022
---
### Major Contributions
- We propose to use paraphrases with alignments between words as a tool for studying how BERT represents words and phrases
- contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different representations in many cases.
---
### Secondary Contribution
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- BERT gives less contextualized representations to paraphrased words than non-paraphrased words, with the exception of punctuation.
- We use the Paraphrase Database (PPDB, Ganitkevitch et al., 2013; Pavlick et al., 2015), a database of paraphrases collected using bilingual pivoting, the process of taking a particular English phrase, looking at all the foreign language phrases it can be translated into, finding all occurrences of these foreign language phrases, and then translating them back into English (Bannard and Callison-Burch, 2005).
---
