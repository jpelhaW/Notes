### Using Paraphrases to Study Properties of Contextual Embeddings
---
- [Zotero Select Link](zotero://select/groups/2480461/items/HEEV8SK6)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/HEEV8SK6)
- Authors: [[Laura Burdick]] [[Rada Mihalcea]] 
- Topics: [[nlp_embeddings]] [[nlp_paraphrase]]
- Venue: #NAACL 
- Year: #2022
---
### Major Contributions
- We propose to use paraphrases with alignments between words as a tool for studying how BERT represents words and phrases
- contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different representations in many cases.
---
### Secondary Contribution
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- BERT gives less contextualized representations to paraphrased words than non-paraphrased words, with the exception of punctuation.
---
