
### Language Models are Few-Shot Learners
---
- [Zotero Select Link](zotero://select/groups/2480461/items/VCN5QM27)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/VCN5QM27)
- Author: [[Tom B. Brown]] [[Benjamin Mann]] [[Nick Ryder]] [[Melanie Subbiah]] [[Dario Amodei]]
- Topics: [[NLP]] [[language_models]]
- Venue: #arXiv 
- Year: #2020
---

### Techniques
- Basically the same training architecture as [[GPT]]-2.
- Much more data (e.g., the [[CommonCrawl]] dataset) 
- Larger model
---

### Goal

To pre-train a model on large amounts of data that can perform almost any task with only a small number of examples.

---

### Major Contributions
- A substantially larger model (than [[_2019_BERT]] or [[GPT]]-2) that is trained on large corpora.
- Analysis on zero-shot, one-shot, and few-shot setups.
---

### Secondary Contribution
-  Detailed analysis of in-context tasks (e.g., arithmetic calculations, word scrambling).
---

### Limitations/Future Work
- Who can run such a large model? It has to be split among multiple GPUs to even run inference.
- Energy consumption for training is quite high, but inference can be done cheaply (e.g., 100 pages of text generation for 0.4 kWh). The authors refer to the [[_2019_GreenAI]] paper that they should be aware of cost and energy use as well as to [[2019 - Improving multi-task deep neural
networks via knowledge distillation for natural language understanding]] for distilling knowledge in smaller inference models.
---

### Notes (Try to use backlinks)
---
