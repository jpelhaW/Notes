# Possible Contributions
---
- Scaling power law of [[_2020_Scaling_laws_for_neural_LMs]] is proportional to model size. If the model size is not relevant but instead the "knowledge" stored in those models (e.g., DistillBERT is a fraction smaller but only a few percent points off in metrics) can we employ a measure to assess this "knowledge" instead of just size?
- Alternative ways to embed/import prior structured knowledge to the neural network model. Can we distance ourselves from the size factor by bringing curated data-knowledge? It does not need to be a ground truth, but to provide us leverage, semantic wise (approximate to few-shot perspective). In this direction, can we guarantee/investigate the "proper" way to initialize our sub-networks? Does everything needs to be randomly initialized?