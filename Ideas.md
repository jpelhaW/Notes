# Possible Contributions
---
- Scaling power law of [[_2020_Scaling_laws_for_neural_LMs]] is proportional to model size. If the model size is not relevant but instead the "knowledge" stored in those models (e.g., DistillBERT is a fraction smaller but only a few percent points off in metrics) can we employ a measure to assess this "knowledge" instead of just size?