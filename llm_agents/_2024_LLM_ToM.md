### Evaluating Large Language Models in Theory of Mind Tasks
---
- [Zotero Select Link](zotero://select/groups/2480461/items/HWA6YIDT)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/HWA6YIDT)
- Authors: [[Michal Kosinski]]
- Topics: [[nlp_llm]] [[nlp_agent]]
- Venue: #arxiv #Stanford
- Year: #2024

---
### Major Contributions
- [Link](https://osf.io/csdhb/)
- We used two types of these tasks (41): Unexpected Contents tasks (42) introduced in Study 1 and Unexpected Transfer tasks (43) introduced in Study 2
- “n Study 3, we administer all tasks to 11 LLMs: GPT-1 (44), GPT-2 (45), six models in the GPT-3 family, ChatGPT-3.5-turbo (21), ChatGPT-4 (46), and Bloom (47)” (Kosinski, 2023, p. 3)
	- “GPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks; ChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of six-year-old children” (Kosinski, 2023, p. 1)
---
### Secondary Contribution
- 
---
### Limitations/Future Work
- 
---
### Notes (Try to use backlinks)
- “Eleven Large Language Models (LLMs) were assessed using a custom-made battery of false-belief tasks, considered a gold standard in testing Theory of Mind (ToM) in humans. The battery included 640 prompts spread across 40 diverse tasks, each one including a false-belief scenario, three closely matched true-belief control scenarios, and the reversed versions of all four.” (Kosinski, 2023, p. 1)
- [[Theory of Min|ToM]] : Many animals excel at using cues such as vocalization, body posture, gaze, or facial expression to predict other animals’ behavior and mental states. “Yet, humans do not merely respond to observable cues, but also automatically and effortlessly track others’ unobservable mental states, such as their knowledge, intentions, beliefs, and desires (2). This ability—typically referred to as “theory of mind” (ToM)—is considered central to human social interactions (3), communication (4), empathy (5), self-consciousness (6), moral judgment (7, 8), and even religious beliefs (9).” (Kosinski, 2023, p. 2)
- “equipping AI with ToM remains one of the grand challenges of our times according to Science Robotics (27) and a vibrant area of research in computer science (28).” (Kosinski, 2023, p. 2)
- “in humans, ToM likely emerged as a byproduct of increasing language ability (4), as indicated by the high correlation between ToM and language aptitude, the delayed ToM acquisition in people with minimal language exposure (37), and the overlap in the brain regions responsible for both (38).” (Kosinski, 2023, p. 3)
- “Even critics of our approach concede that the performance of LLMs on ToM tasks is poised to improve (60). Thus, even if we reject our results as wholly unconvincing, future models will likely decisively outperform humans, not just in false-belief tasks, but also in other tasks and situations that require the ability to infer unobservable mental states.” (Kosinski, 2023, p. 20)
---
