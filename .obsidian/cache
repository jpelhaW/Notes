{"files":{"_template.md":{"mtime":1595939515362.9724,"size":343,"hash":"8fc520817b0585ad3506b4344926e5ccea489df78c5246dc23ec85d51a15d45c"},"research/2020 - (Re)construing Meaning in NLP.md":{"mtime":1595862332000,"size":5097,"hash":"48082715cb90becc5e6e95b314093c4fe596ff9982cb7bdf0f9c56535565e4a8"},"research/2020 - Contextual Embeddings When Are TheyWorth It.md":{"mtime":1595863090000,"size":5188,"hash":"3df95b7fa77eef6ba9af1e0a26dcb9666e83ce273b2854d79d077685a552315f"},"research/2020 - Dont Stop Pretraining Adapt Language Models to Domains and tasks.md":{"mtime":1595939443000,"size":725,"hash":"9817337c3fc4b0b112c6207e6a982cc98a44512c0f4f51fd26dbcbc54e2c8904"},"research/2020- GilBERT Towards pre-trained word-sense disambiguation with bidirection transformers and lexical databases.md":{"mtime":1595939695138.9492,"size":364,"hash":"7644a8f46512197571f30ae99301a36cab38da81d07d15841f962474b030d8fe"}},"metadata":{"48082715cb90becc5e6e95b314093c4fe596ff9982cb7bdf0f9c56535565e4a8":{"links":[{"line":1,"link":"Sean Trott","original":"[[Sean Trott]]","displayText":"","beforeContext":"Auhtor: ","afterContext":" Nathan Schneider"},{"line":1,"link":"Nathan Schneider","original":"[[Nathan Schneider]]","displayText":"","beforeContext":"Auhtor: Sean Trott ","afterContext":""},{"line":2,"link":"nlp_theory","original":"[[nlp_theory]]","displayText":"","beforeContext":"Tags: ","afterContext":" semantics NLP"},{"line":2,"link":"semantics","original":"[[semantics]]","displayText":"","beforeContext":"Tags: nlp_theory ","afterContext":" NLP"},{"line":2,"link":"NLP","original":"[[NLP]]","displayText":"","beforeContext":"Tags: nlp_theory semantics ","afterContext":""},{"line":8,"link":"theory","original":"[[theory]]","displayText":"","beforeContext":"","afterContext":""}],"embeds":[],"tags":[{"line":3,"tag":"#ACL"},{"line":23,"tag":"#perspective"},{"line":23,"tag":"#prominence"},{"line":23,"tag":"#resolution"},{"line":23,"tag":"#configuration"},{"line":23,"tag":"#metaphor"}],"headings":[{"line":7,"heading":"Techniques","level":3},{"line":11,"heading":"Goal","level":3},{"line":15,"heading":"Major Contributions","level":3},{"line":19,"heading":"Secondary Contributions","level":3},{"line":22,"heading":"Limitations/Future Work","level":3},{"line":26,"heading":"Notes","level":3}]},"3df95b7fa77eef6ba9af1e0a26dcb9666e83ce273b2854d79d077685a552315f":{"links":[{"line":1,"link":"Simran Arora","original":"[[Simran Arora]]","displayText":"","beforeContext":"Auhtor: ","afterContext":" Christopjer Ré"},{"line":1,"link":"Christopjer Ré","original":"[[Christopjer Ré]]","displayText":"","beforeContext":"Auhtor: Simran Arora ","afterContext":""},{"line":2,"link":"nlp_embeddings","original":"[[nlp_embeddings]]","displayText":"","beforeContext":"Tags: ","afterContext":" nlp_transformers"},{"line":2,"link":"nlp_transformers","original":"[[nlp_transformers]]","displayText":"","beforeContext":"Tags: nlp_embeddings ","afterContext":""},{"line":8,"link":"BERT","original":"[[BERT]]","displayText":"","beforeContext":"","afterContext":""},{"line":9,"link":"GloVe","original":"[[GloVe]]","displayText":"","beforeContext":"","afterContext":""}],"embeds":[],"tags":[{"line":3,"tag":"#ACL"}],"headings":[{"line":7,"heading":"Techniques","level":3},{"line":12,"heading":"Goal","level":3},{"line":16,"heading":"Major Contributions","level":3},{"line":19,"heading":"Secondary Contributions","level":3},{"line":22,"heading":"Limitations/Future Work","level":3},{"line":25,"heading":"Notes","level":3}]},"8fc520817b0585ad3506b4344926e5ccea489df78c5246dc23ec85d51a15d45c":{"links":[{"line":4,"link":"first author","original":"[[first author]]","displayText":"","beforeContext":"Auhtor: ","afterContext":" last auhtor relevant other"},{"line":4,"link":"last auhtor","original":"[[last auhtor]]","displayText":"","beforeContext":"Auhtor: first author ","afterContext":" relevant other"},{"line":4,"link":"relevant other","original":"[[relevant other]]","displayText":"","beforeContext":"Auhtor: first author last auhtor ","afterContext":""},{"line":5,"link":"topic1","original":"[[topic1]]","displayText":"","beforeContext":"Topics: ","afterContext":" #topic2"},{"line":5,"link":"#topic2","original":"[[#topic2]]","displayText":"","beforeContext":"Topics: topic1 ","afterContext":""}],"embeds":[],"tags":[{"line":6,"tag":"#venue"},{"line":7,"tag":"#year"}],"headings":[{"line":9,"heading":"Techniques","level":3},{"line":13,"heading":"Goal","level":3},{"line":15,"heading":"Major Contributions","level":3},{"line":17,"heading":"Secondary Contribution","level":3},{"line":19,"heading":"Limitations/Future Work","level":3},{"line":21,"heading":"Notes (Try to use backlinks)","level":3}]},"7644a8f46512197571f30ae99301a36cab38da81d07d15841f962474b030d8fe":{"links":[{"line":4,"link":"first author","original":"[[first author]]","displayText":"","beforeContext":"Auhtor: ","afterContext":" last auhtor relevant other"},{"line":4,"link":"last auhtor","original":"[[last auhtor]]","displayText":"","beforeContext":"Auhtor: first author ","afterContext":" relevant other"},{"line":4,"link":"relevant other","original":"[[relevant other]]","displayText":"","beforeContext":"Auhtor: first author last auhtor ","afterContext":""},{"line":5,"link":"topic1","original":"[[topic1]]","displayText":"","beforeContext":"Topics: ","afterContext":" #topic2"},{"line":5,"link":"#topic2","original":"[[#topic2]]","displayText":"","beforeContext":"Topics: topic1 ","afterContext":""}],"embeds":[],"tags":[{"line":6,"tag":"#venue"},{"line":7,"tag":"#year"}],"headings":[{"line":9,"heading":"Techniques","level":3},{"line":11,"heading":"Goal","level":3},{"line":14,"heading":"Major Contributions","level":3},{"line":16,"heading":"Secondary Contribution","level":3},{"line":18,"heading":"Limitations/Future Work","level":3},{"line":20,"heading":"Notes (Try to use backlinks)","level":3}]},"9817337c3fc4b0b112c6207e6a982cc98a44512c0f4f51fd26dbcbc54e2c8904":{"links":[{"line":1,"link":"Suchin Gururangan","original":"[[Suchin Gururangan]]","displayText":"","beforeContext":"Auhtor: ","afterContext":" Noah Smith Iz Beltagy"},{"line":1,"link":"Noah Smith","original":"[[Noah Smith]]","displayText":"","beforeContext":"Auhtor: Suchin Gururangan ","afterContext":" Iz Beltagy"},{"line":1,"link":"Iz Beltagy","original":"[[Iz Beltagy]]","displayText":"","beforeContext":"Auhtor: Suchin Gururangan Noah Smith ","afterContext":""},{"line":2,"link":"nlp_lm","original":"[[nlp_lm]]","displayText":"","beforeContext":"Topics: ","afterContext":" nlp_task nlp_lm"},{"line":2,"link":"nlp_task","original":"[[nlp_task]]","displayText":"","beforeContext":"Topics: nlp_lm ","afterContext":" nlp_lm"},{"line":2,"link":"nlp_lm","original":"[[nlp_lm]]","displayText":"","beforeContext":"Topics: nlp_lm nlp_task ","afterContext":""},{"line":9,"link":"Pretraining","original":"[[Pretraining]]","displayText":"","beforeContext":"They show a second ","afterContext":" in domain domain-adaptative pretraining (DAPT) leads to performance gains, under both high and low resource settings."},{"line":14,"link":"TAPT","original":"[[TAPT]]","displayText":"","beforeContext":"Highlight of how task-adaptative pretraining ","afterContext":" and DAPT"},{"line":14,"link":"DAPT","original":"[[DAPT]]","displayText":"","beforeContext":"Highlight of how task-adaptative pretraining TAPT and ","afterContext":""},{"line":17,"link":"TAPT","original":"[[TAPT]]","displayText":"","beforeContext":"","afterContext":" is even better when unlabeled data from the task distribution"}],"embeds":[],"tags":[{"line":3,"tag":"#ACL"}],"headings":[{"line":6,"heading":"Techniques","level":3},{"line":8,"heading":"Goal","level":3},{"line":12,"heading":"Major Contributions","level":3},{"line":16,"heading":"Secondary Contribution","level":3},{"line":19,"heading":"Limitations/Future Work","level":3},{"line":21,"heading":"Notes (Try to use backlinks)","level":3}]}},"algorithmVersion":9}