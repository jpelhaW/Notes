### Add title here
---
- [Zotero Select Link](zotero://select/groups/2480461/items/6SBFRQCR)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/6SBFRQCR)
- Authors: [[Yuchen Liang]] [[Dimitry Krotov]]
- Topics: [[nlp_embeddings]] [[nlp_transformers]]
- Venue: #arXiv 
- Year: #2021
---
### Major Contributions
- It is shown that not only can the fruit fly network motif achieve performance comparable to existing methods in NLP, but, additionally, it uses only a fraction of the computational resources (shorter training time and smaller memory footprint)
- can the correlations between words and their contexts be extracted from raw text by the biological network of KCs,
---
### Secondary Contribution
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- Our approach relies on a recent proposal that the recurrent network of mutually inhibited KCs can be used as a “biological” model for generating sparse binary hash codes for the input data presented at the projection neuron layer [8].
- ![[2021_FruitFly_example.png]]
- both context-dependent and static embeddings are mapped into the same space of sparse binary hash codes (a vector of elements, with ones in it).
- A nice aspect of binary embeddings is that they result in tighter and better separated clusters than continuous embeddings
- 
---
