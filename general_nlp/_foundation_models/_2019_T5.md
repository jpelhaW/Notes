### Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
---
- [Zotero Select Link](zotero://select/groups/2480461/items/F8DGISB7)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/F8DGISB7)
- Authors: [[Colin Raffel]] [[Noam Shazeer]] [[Peter J. Liu]]
- Topics: [[nlp_transformers]] [[nlp_ann]] [[nlp_transfer_learning]]
- Venue: #arXiv #JMLR
- Year: #2019
---
### Major Contributions
- Comparee pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks
---
### Secondary Contribution
- They treat every text processing problem as a text-to-text problem (text-input -> text-output)
- 
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- Common Crwal project produces about 20TB of text data extracted from web pages each month
	- a lot of noise comes with this dataset
	- a list of pre-processing steps is provided in page 06. > 5 sentences, >2 words, no code pages, no placeholder, etc
- They use relative postion embeddings instead of absolute
- [[_2019_T5]] diagram
	- ![[2019_T5_diagram.png]] 
- They use the [[C4|Colossal Clean Crawled Corpus]] for their study
- Similar architecture to [[_2017_AttentionIsAllYouNeed]] except they remove the Layer Norm bias, placing the normalization outside the residual path + relative positional scheme
- Explore several tasks: [[GLUE]] [[COPA]] [[MNLI]] [[SST-2]] [[SuperGLUE]] [[_2019_Superglue]] [[QQP]] [[MultiTRC]] [[WNLI]] [[WSC]] [[WIC]] [[RECORD]][[CoLA]]
- 
---
