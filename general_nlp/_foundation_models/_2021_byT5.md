### ByT5: Towards a token-free future with pre-trained byte-to-byte models
---
- [Zotero Select Link](zotero://select/groups/2480461/items/9WSDZ9LK)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/9WSDZ9LK)
- Authors: [[Linting Xue]] [[Colin Raffel]]
- Topics: [[nlp_lm]] [[nlp_transformers]]
- Venue: #arXiv 
- Year: #2021
---
### Major Contributions
- we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences
	-  We release ByT5 in five sizes analogous to T5 and mT5 (Small, Base, Large, XL, XXL)
-  ByT5 is competitive with parameter matched mT5 models that rely on SentencePiece vocabulary. Specifically, ByT5 outperforms mT5 in any of these four scenarios: (1) at model sizes under 1 billion parameters, (2) on generative tasks, (3) on multilingual tasks with in-language labels and (4) in the presence of various types of noise.
---
### Secondary Contribution
- Improvements over related work - Our work differs from these in that 
	- (a) we train encoder-decoder models that extend to generative tasks,
	-  (b) our models work directly with UTF-8 bytes, and 
	-  (c) we explore the effect of model scale, training models beyond 10 billion parameters.
---
### Limitations/Future Work
- The main drawback of byte-level models is that byte sequences tend to be significantly longer than token sequences. For example, given that the average word length in English is about five characters, an English-language byte or character sequence will typically be about five times longer than the corresponding word-level token sequence
- We believe techniques such as hash embeddings, local attention and down-sampling (Clark et al., 2021), as well as sparse computation (Fedus et al., 2021) can help address latency issues, removing the remaining barriers to a token-free future.
---
### Notes (Try to use backlinks)
- An issue with using a fixed vocabulary of words is that there is no obvious way to process a piece of text that contains an out-of-vocabulary word.
	- subword tokenizers are a good alternative (not ideal solution as problems still exist)
- In this work, we make use of the fact that text data is generally stored as a sequence of bytes. Therefore, feeding byte sequences directly into the model would allow the model to process arbitrary sequences of text
	- The large vocabularies of word- or subword level models often result in many parameters being devoted to the vocabulary matrix. In contrast, a byte-level model by definition only requires 256 embeddings.
	- ByT5 is competitive with a subword-level baseline,despite being pre-trained on 4 times less text.
- we find that masking longer byte-spans is valuable. Specifically, we set our mean mask span length to 20 bytes
- Third, we find that ByT5 performs best when we decouple the depth of the encoder and decoder transformer stacks. While T5 and mT5 used “balanced” architectures, we find byte-level models benefit significantly from a “heavier” encoder. Specifically, we set our encoder depth to 3 times that of the decoder.
	- By decreasing decoder capacity, one might expect quality to deteriorate on tasks like summarization that require generation of fluent text. However, we find this is not the case, with heavy encoder byte models performing better on both classification and generation tasks.
- a byte level model trained on the same number of tokens as a word- or subword-level model will have been trained on less text data.
- We observe that [[_2021_mT5]] degrades more in the presence of noise than [[_2021_byT5]], across all six noise conditions.
- Across all model sizes, ByT5 requires roughly 1.2x more operations, resulting in roughly 0.75x as many sequences per second.
- Due to wider transformer layers and increased sequence lengths, ByT5 is between 1.1 to 2.0 times slower for classification and between 1.5 to 7.0 times slower for generation, depending on the model size.
- ![[2021_byT5_example.png]]
---
