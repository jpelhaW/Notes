### Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding
---
- [Zotero Select Link](zotero://select/groups/2480461/items/SVWUGFJF)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/SVWUGFJF)
- Author: [[Xiaodong Liu]] [[Jianfeng Gao]]
- Topics: [[nlp_ann]] [[nlp_lm]] [[nlp_multi_task]]
- Venue: #arXiv 
- Year: #2019
---
### Techniques
- 
---
### Goal
---
### Major Contributions

Applies the Knowledge Distillation proposed by [[2015 - Distilling the Knowledge in a Neural Network]] to multi-task learning in NLP with neural networks.

---
### Secondary Contribution
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
---