### Taking Notes on the Fly Helps Language Model Pre-Training
---
- [Zotero Select Link](zotero://select/groups/2480461/items/2SE3DPZH)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/2SE3DPZH)
- Authors: [[Qiyu Wu]] [[Tie-Yan Liu]] 
- Topics: [[nlp_transformers]] [[nlp_lm]] [[_2019_BERT]]
- Venue: #arXiv 
- Year: #2020
---
### Major Contributions
- Incorporating dictionary entries for rare words improve the[[LM]] - over [[_2019_BERT]]
- [[_2020_TNF]] proposes to maintain a note dictionary where the keys are rare words and the values are historical contextual representations of them
	- the note dictionary is only used during training
- 
---
### Secondary Contribution
- How to make languagem model pre training more efficient and less resource intensive (Strubell 2019)
- [[_2020_TNF]] can be applied to any pre-training [[LM]] 
- To verify whether the Transformer encoder is better trained with TNF, they removed #NoteDict  for fine-tuning and only use the trained encoder in the downstream tasks.
---
### Limitations/Future Work
- They state around 20% of the senteces in the corpus contain at least one rare word. No information from which corpus this observation was derived.
- ""*TNF’s training time is 60% less than BERT when reaching the same performance.*"". Not sure.
---
### Notes (Try to use backlinks)
- [[heavy-trail distribution]] (Larson 2010) #TODO
- Memory augumented BERT - use memory augumented neural networks during training. (Févry 2020) (Guu 2020) define memory buffer as an external knowledge base of entities for better open domain question answering tasks.
- A dictiornay called #NoteDict is built from their data corpus
	- which valures are randomly initialized as word/positional embeddings
- The input embedding had [position + token] + [note_dict] if the rare word exists otherwise just [position + token]
- ![[2020_TNF_training_framework.png]]
- Use Wikipedia and Bookcorpus for pre-training
- Tasks: #GLUE
- They will try to send this paper to #ACL #2021 
---
