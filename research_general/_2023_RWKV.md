### RWKV: Reinventing RNNs for the Transformer Era
---
- [Zotero Select Link](zotero://select/groups/2480461/items/RYRWINKQ)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/RYRWINKQ)
- Authors: [[First Author]] [[Important Author]] [[Rui-Jie Zhu]] 
- Topics: [[ann_rnn]] [[nlp_rnn]]
- Venue: #arXiv
- Year: #2023

---
### Major Contributions
- We propose a novel model architecture, [[Receptance Weighted Key Value|RWKV|]] (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. 
	- [[RWKV]] is carefully designed to alleviate the memory bottleneck and quadratic scaling associated with Transformers
		- **R: Receptance** vector acting as the acceptance of past information.
		- **W : Weight** is the positional weight decay vector. A trainable model parameter.
		- **K: Key** is a vector analogous to K in traditional attention.
		- **V : Value** is a vector analogous to V in traditional attention.
	- We propose a new attention mechanism reformulation that results in linear attention, eschewing the quadratic complexity associated with standard Transformer models.
	- Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters.
- **Questions**
	- **RQ1**: Is RWKV competitive against quadratic transformer architectures with equal number of parameters and training tokens?
	- **RQ2:** When increasing the number of parameters, does RWKV remain competitive against quadratic transformer architectures?
	- **RQ3:** Does increasing parameters of RWKV yield better language modeling loss, when RWKV models are trained for context lengths that most open-sourced quadratic transformers cannot efficiently process?
	- Addressing RQ1 and RQ2, from Fig. 4, we can see that RWKV is very competitive on six benchmarks (Winogrande, PIQA, ARC-C, ARC-E, LAMBADA, and SciQ) against major open source quadratic complexity transformer models
---
### Secondary Contribution
- 
---
### Limitations/Future Work
- the linear attention of RWKV leads to significant efficiency gains but still, it may also limit the model’s performance on tasks that require recalling minutiae information over very long contexts.
- The linear attention mechanism used in RWKV limits the information from the prompt that will be carried over to the model’s continuation. As a result, carefully designed prompts may be even more crucial for the model to perform well on tasks
---
### Notes (Try to use backlinks)
- Transformers suffer from memory and computational complexity that scales quadratically with sequence length.
- We propose a novel model architecture, Receptance Weighted Key Value [[RWKV]], that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs.
	- Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters.
- The implementation of linear attention in RWKV is carried out without approximation, which offers a considerable improvement in efficiency and enhances the scalability
- While [[QRNN]] utilizes convolutional filters with fixed sizes, [[RWKV]] employs a time-mixing module as an attention mechanism with time-decaying factors. Different from the element-wise pooling in [[QRNN]], [[RWKV]] includes a parametrized channel-mixing module (see the green blocks in Fig.1c) that is parallelizable
- Inspired by AFT, we let each wt,i in RWKV be a channel-wise time decay vector multiplied by the relative position, traced backwards from current time as it decays
- The RWKV architecture is comprised of a series of stacked residual blocks, each formed by a timemixing and a channel-mixing sub-blocks
- The RWKV model features a single-step process for updating attention-like scores, which includes a time-dependent softmax operation that helps numerical stability and guards against vanishing gradients (for rigorous proof, see Appendix F). 
	- Takes care of vanishing and exploding gradient problems
- RWKV captures and propagates sequential information through the combination of three mechanisms: recurrence, time decay and token shift.
	- The **recurrence** in the time-mixing block of [[RWKV]] is the basis for the model’s capacity to capture intricate relationships between sequence elements and to propagate locality information through time. 
	- The **time decay** mechanism (e−w and eu in equation 14), maintains sensitivity to the positional relationship between sequence elements.
		- This treatment of positional information in sequential data exhibits similarities to the Attention with Linear Biases ([[ALiBi]]) model (Press et al., 2022), where the linear biases facilitate input length extrapolation
			- In this context, the RWKV architecture can be perceived as a trainable version of ALiBi
	- The **token shift** or **time-shift mixing**, or (diagonal arrows in Figure 3), also contributes to the model’s adaptation to sequential data. By linearly interpolating between the current input and the previous time step input, the model naturally aggregates and gates information in the input channels.
- ![[2023_RWKV_architecture.png]]
- ![[2023_RWKV_block_elements.png]]
---

