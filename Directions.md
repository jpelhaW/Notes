# Directions
---

The Directions connect our backlinked [[Idea|ideas]] to actual research experiments that can be done.

- Replicate the study [[_2020_Scaling_laws_for_neural_LMs]] for NLP and CV and target the explicit observations such as architectural changes are not significantly relevant.
- Replicate the study of [[_2019_Growing_a_Brain]] for the NLP (transformer) domain.
- Use the dataset of ACL to analyze who dominates AI and extend it with semantic features. Major benefit: we can have a paper proposing the dataset (with hopefully many citations).
- Alternative ways to embed/import prior structured knowledge to the neural network model. Can we distance ourselves from the size factor by bringing curated data-knowledge? It does not need to be a ground truth, but to provide us leverage, semantic wise (approximate to few-shot perspective). In this direction, can we guarantee/investigate the "proper" way to initialize our sub-networks? Does everything needs to be randomly initialized?