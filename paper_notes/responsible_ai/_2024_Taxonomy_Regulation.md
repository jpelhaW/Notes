### Taxonomy to Regulation A Geo Politcal Taxonomy for AI Risks and Regulatory Measures in the EU AI Act
---
- [Zotero Select Link](zotero://select/groups/2480461/items/4U6XLRGE)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/4U6XLRGE)
- Authors: [[Sinan Arda]]
- Topics: [[eu_ai_act]] [[ai_responsibe]]
- Venue: #arxiv
- Year: #2024

---
### Major Contributions
- “This work proposes a taxonomy focusing on on (geo)political risks associated with AI. It identifies 12 risks in total divided into four categories: (1) Geopolitical Pressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks, and (4) Privacy and Trust Violations.” (Arda, 2024, p. 1)
	- “two research questions guide the research design: **(RQ1) What kind of risks does AI pose for domestic and international politics?** **(RQ2) How does the European Union Artificial Intelligence Act mitigate the risks associated with AI?”** (Arda, 2024, p. 1)
		- “n the case of RQ2, the EU’s legislative process toward the AI Act will be analyzed by referring to the first three stages of the policy cycle. Afterward, the regulatory loopholes of the present agreement will be discussed.” (Arda, 2024, p. 2)
---
### Secondary Contribution
- “The Draft Agreement defines a GPAI model as "an AI model, including when trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable to competently perform a wide range of distinct tasks regardless of the way the model is placed on the EU market and that can be integrated into a variety of downstream systems or applications" (Article 3, 44b Final Draft).” (Arda, 2024, p. 12) -> 
	- This scenario will be the standard the moment we run out of data and use AI to generate more. In terms the AI will supervise itself.
- “The term "foundation model" resembles what is known in the AI Act as GPAI models, i.e. AI systems characterized by expansive capabilities adaptable to various specific purposes. The terminology is often used synonymous with GPAI and LLM given that LLMs such as ChatGPT are the contemporary example for systems with multi-use capabilities. For the November 2022 release of ChatGPT, an LLM called GPT-3.5 served as the foundation model. See Friedland [85] for a clarification of the interchangeably used terminologies.” (Arda, 2024, p. 13)
	- “Alex Friedland. What Are Generative AI, Large Language Models, and Foundation Models?, 2023. URL https://cset.georgetown.edu/article/what-are-generative-ai-large-language-modelsand-foundation-models/.” (Arda, 2024, p. 19)
---
### Limitations/Future Work
- 
---
### Notes (Try to use backlinks)
- “the question of AI governance has become a policy priority and led to political agreements. In October 2023, President Biden issued an Executive Order intending to manage AI risks [3]. Two months later, the EU, after over two years of drafting time and inter-institutional negotiations, has reached an initial political agreement on the EU Artificial Intelligence Act (AI Act),” (Arda, 2024, p. 1)
- “Member States are likely to face costly policies that they have to "download" from the European to the national level [6]. Europeanization is a two-way process that entails both a "bottom-up" and "top-down" dimension. While the first dimension describes the evolution of the European institutions as a set of new norms, rules, and practices, the second one describes the institution’s impact on the political structures and processes of the Member States.” (Arda, 2024, p. 2)
- “As the EU AI Act is yet to be implemented, only the first three stages are of interest, i.e., problem definition and agenda setting, policy formulation, and decision-making” (Arda, 2024, p. 2)
- “the EU is able to regulate global markets by setting standards that shape the international business environment. Market forces alone are often sufficient to lead to a Europeanization of global commerce as international corporations voluntarily comply with EU law when governing their global operations. The EU’s General Data Protection Regulation (GDPR) has become a well-known empirical example of the Brussels Effect as it emerged as the default for data privacy regulation, while the AI Act’s potential for a similar effect has been playing an important role in the legislative debates surrounding AI [15, 16].” (Arda, 2024, p. 3)
- “The assessment of AI risks is conducted in a three-step process. Firstly, the risk landscape of AI will be mapped through a literature review of existing risk taxonomies. Secondly, a risk taxonomy for risks that can be situated in the political setting is proposed. Thirdly, selected risks of the proposed taxonomy will be analyzed in detail” (Arda, 2024, p. 3)
- Google DeepMind Taxonomy
	- “The taxonomy categorizes risks into six thematic domains: 1. Discrimination, Hate speech, and Exclusion, 2. Information Hazards, 3. Misinformation Harms, 4. Malicious Uses, 5. Human-Computer Interaction Harms, and 6. Environmental and Socioeconomic harms [18]” (Arda, 2024, p. 3)
	- “Their holistic risk taxonomy of generative AI systems incorporates findings of the 2022 LLMs assessment and focuses on: 1. Representation & Toxicity Harms, 2. Misinformation Harms, 3. Information & Safety Harms, 4. Malicious Use, 5. Human Autonomy & Integrity Harms, 6. Socioeconomic & Environmental Harms [19].” (Arda, 2024, p. 3)
- ![[2024_Taxonomy_Regulation_Risks_AI.png]]
- “Closely following the Cold War rationale, an AI Arms Race describes how the technology bears a risk of becoming "a way for nations to flex on one another" [28].” (Arda, 2024, p. 6) 
	- more like companies
- “New forms of misinformation and disinformation have been described as potentially being the most significant dangers that AI will unleash by Mustafa Suleyman, the co-founder of the leading AI firm DeepMind [35]. Thereby, LLMs will be at the center of abuse potential.” (Arda, 2024, p. 6)
- “LLMs can reduce the cost of disinformation campaigns as they can create synthetic media output at a large scale. In particular, the creation of wrong majority opinions and the disruption of online discourses have already manifested themselves as the most significant LLM disinformation risks, according to findings in a recent DeepMind research project [37].” (Arda, 2024, p. 6)
- Environmental, social and ethical risks: “It focuses on AI’s (i) environmental impact, (ii) economic risks linked to workforce pressures, as well as (iii) bias, discrimination, and accountability problems.” (Arda, 2024, p. 7)
- “Unethical AI, subject to bias and discrimination based on training data reflecting societal issues, could intensify unrest if left unsolved [44, 37]. As the world is experiencing an AI hype circle and the technology is likely to be incorporated cross-sectoral with yet unforeseeable consequences, more work on environmental efficiency and ethical AI is an important and necessary step for an AI-dominated planet.” (Arda, 2024, p. 7)
- “Authoritarian regimes can thus rely on AI to strengthen their surveillance capabilities and the capacity to exercise coercive power, such as tracking specific individuals. Notably, China, an export leader in surveillance technology, already has a track record of using AI for authoritarian surveillance practices such as the observation of political dissidents or the repression of the Uyghur and Turkic Muslim populations” (Arda, 2024, p. 7)
- “In times when AI provides a way for governments to carry out more censorship, maintaining the delicate balance between the removal of illegal content and the guarantee of freedom of speech has become more complex.” (Arda, 2024, p. 7)
- “Even if the geopolitical competition in the AI arena might not constitute an arms race, an argument often put forward [55, 56, 57], its dynamics can be described as a security dilemma in which the quest for security could lead to insecurity. New technological developments, from the longbow to nuclear weapons, have always created some level of uncertainty about military capabilities as it is hard to predict how a technology will be utilized, which exacerbates the security dilemma.” (Arda, 2024, p. 8)
- “**OpenAI’s rather quiet update of its usage policies for ChatGPT in January 2024, which lifted a blanket ban on using the technology for "military and warfare" purposes, suggests the potential increasing importance of harvesting already-sophisticated AI systems for military means [62].” (Arda, 2024, p. 8)**
	- **“[62] Sam Biddle. OpenAI Quietly Deletes Ban on Using ChatGPT for “Military and Warfare”, 2024. URL https: //theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/.” (Arda, 2024, p. 18)**
- “Based on the collective intelligence of almost 1500 risk analysts, the report’s findings deemed **misinformation and disinformation the most severe immediate risk.**” (Arda, 2024, p. 9)
- “Responding to the public attention given to ChatGPT, the European Union Agency for Law Enforcement Cooperation (Europol) assessed the use of sophisticated LLMs for criminal practices. In its report, Eurpol underlined that LLMs excel at drafting authentic-looking text at speed at scale [73].” (Arda, 2024, p. 9)
	- [73] “Europol. ChatGPT: The Impact of Large Language Models on Law Enforcement. Publications Office of the European Union, 2023. URL https://data.europa.eu/doi/10.2813/255453.” (Arda, 2024, p. 19)
	- [74] “Josh A. Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. How persuasive is AIgenerated propaganda? PNAS Nexus, 3(2), 2024. doi:10.1093/pnasnexus/pgae034.” (Arda, 2024, p. 19)
- Proposal EU Commission
	- “The Proposal’s primary objective is to establish a legal framework for secure, trustworthy, and ethical AI. **AI systems exclusively designed for military purposes are excluded from any regulatory obligations (Title 1, Article 2, Proposal).” (Arda, 2024, p. 10)**
	- “broad definition of AI systems as "software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with" (Title I, Article 3, Proposal).” (Arda, 2024, p. 10)
	- “The cornerstone of the Proposal is the implementation of a risk-based classification scheme that distinguishes between different AI usage classes and risk levels to evaluate the potential risks AI could pose to individuals’ health and safety or fundamental rights.” (Arda, 2024, p. 10)
	- “Examples of prohibited practices captured in Title II Article 5 include the placing on the market, putting into service, or use of an AI system that enables the following: manipulation of individuals through subliminal stimuli beyond their consciousness and the exploitation of vulnerabilities of specific groups, such as children or persons with disabilities, to distort their behavior in a way that causes themselves or others psychological or physical harm.” (Arda, 2024, p. 11) -> how is this any different from what current propaganda/TV/newspapers/youtube/etc does?
	- “The Proposal differentiates between two main categories of high-risk AI systems: (1) AI systems intended to be used as safety components of products subject to third party ex-ante conformity assessment, and (2) AI systems used for sensitive purposes that carry fundamental rights implications and are explicitly listed in the Proposal’s Annex III.” (Arda, 2024, p. 11)
		- “(i) Biometric identification and categorization of natural persons, (ii) Management and operation of critical infrastructure, (iii) Educational and vocational training impacting access to education and career paths, **(iv) Employment-linked technologies such as recruitment, screening, or monitoring software, (v) Essential private and public services like credit scoring for loans or emergency dispatching services, (vi) Law enforcement**, (vii) Migration, asylum and border control management, (viii) Administration of justice and democratic processes.” (Arda, 2024, p. 11)
	- “If an AI system labeled as high risk seeks access to the European market, it must be subject to compliance with the AI Act’s ex-ante conformity assessment and several technical requirements. To allow the free usage of high-risk AI systems across the European market and to display their regulatory compliance, an obligatory Conformité Europénne (CE) marking will indicate the high-risk system’s conformity with the requirements set out in Title III.” (Arda, 2024, p. 11)
		- **“The requirements for providers of high-risk AI systems (Article 8-25) include the implementation of a comprehensive risk-management system throughout the entire life cycle of a high-risk AI system, testing of data sets and data governance, technical documentation, record-keeping, transparency and provision of information to users, human oversight obligations, and standards for accuracy, robustness, and cybersecurity” (Arda, 2024, p. 11)**
	- “Consequently, an AI system is defined as "a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments" (Article 3, Final Draft).” (Arda, 2024, p. 11)
- **Notes**
	- What is the cost of europenization of AI regulations to rich/poor countries
	- Not many discussion how this can cripple the development of prominent actors in AI
	- What is the most dangerous thing(s) unregulated AI development can pose?
	- The existential risks seem to be true for many other things than AI
	- Very poor naming for AWS Autonomous Wepon System
	- Malicious usage, privacy and trust violations are great points
		- Geopolitical, environmental, social,  and ethic reasons seems weak
			- We already have issues today with industrialization and goods productions on certain countries. What do you think it will happens when it comes to AI? Same. Results/Performance/Advantage will weight more than the energy/environment cost
	- When synthetic data is generated/used, who/what will tell what is true or not?
	- The companies arms race seem more realistic than an actual country. Except for china, that owns the companies, most of the racers seem to be distributed/alone
	- the update on use of ai for military purpose is both disturbing and excepted - What about DARPA? no mention to it or what so ever
	- How the prohibitions on Title 2 Article 5 are any different from what people/companies do? Why this is not enforced/pursued today?
	- Liked the idea of disclosing the interaction with AI from the EU Act
	- 10^25 FLOPS to categorize highrisk seems a bit old fashioned (these things get old fast)
		- maybe this should be replaced by a more modular hollistic view - although some categorization on potential risks are interesting
	- Open models are exempt from regulatory restrictions?
	- The contributions is a bit loose in terms of: that's my contribution and that is a summary of things we'read
---
