### "Why Should I Trust You?" Explaining the Predictions of Any Classifier
---
- [Zotero Select Link](zotero://select/groups/2480461/items/IJDU72YX)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/IJDU72YX)
- Authors: [[Marco Tulio Ribeiro]] [[Carlos Guestrin]]
- Topics: [[nlp_metrics]] [[nlp_task]] [[nlp_explain]]
- Venue: #NAACL
- Year: #2016
---
### Major Contributions
- [[_2016_LIME]] - Local Interpretable Model-Agnostic: the goal is to identify an interpretable model over the intepretable representation that is locally faithful to the predictions of any classifier 
	- they also propose [[SP-LIME]] a method that selects a set o representative explanations to address the trusting the model problem, via submodular optimization
---
### Secondary Contribution
- They hope to give a global understanding of the model by explaining a set of individual instances. this should provide a better evaluation of the model as a whole.
- 
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- interpretable representation for text is a binary vector indicating the presence or absece of a word -> the classifier will use more complex/incomprehensive
- They argue that even though classifiers achieve high accuracy (94%), their decision are based on irrelevant tokens to the study object
	- Posting, Host, Re - are used to classify Christianity or Atheism
- Code for [[_2016_LIME]] [here](https://github.com/marcotcr/lime)
- Demo for [[_2016_LIME]] [here](https://github.com/uw-mode/naacl16-demo)
- x - original instance [0,1]
- g - explanation: abscence or presence of the interpretable components
- Omega(g) - complexity of 'g'
- f(x) - probability that 'x' belogns to a certain class
- Pi(z) - proximity measure between an instance z to x - locality of x
- L(f,g,Pi) - measure of how unfaithful 'g' is in approximing 'f' in the locality defined by Pi - which is minimized by the model + Omega (lower dimension - so it can be understood by human) 
- #TODO hard to read
- 
---
