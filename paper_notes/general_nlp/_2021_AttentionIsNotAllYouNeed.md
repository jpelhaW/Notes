### Attention is not all you need: pure attention loses rank doubly exponentially with depth
---
- [Zotero Select Link](zotero://select/groups/2480461/items/MYIST6JC)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/MYIST6JC)
- Authors: [[Yihe Dong]] [[Andreas Loukas]]
- Topics: [[topic1]] [[topic2]]
- Venue: #arXiv 
- Year: #2021
---
### Major Contributions
- Using decomposition, we prove that self-attention possesses a strong inductive bias towards token uniformity
	- skip connections and MLPs stop the output from degeneration.
- 
---
### Secondary Contribution
---
### Limitations/Future Work
---
### Notes (Try to use backlinks)
- This work provides new insights about the operation and inductive bias of networks built by stacking multiple self-attention layers. Surprisingly, we find that pure self-attention networks (SANs), i.e., transformers with skip connections and multi-layer perceptrons (MLPs) disabled, lose expressive power doubly exponentially with respect to network depth
- Our analysis indicates that skip connections play a key role in mitigating rank collapse, and MLPs can slow down the convergence by increasing their Lipschitz constant.
- 
---
