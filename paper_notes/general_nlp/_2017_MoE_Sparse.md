### Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer
---
- [Zotero Select Link](zotero://select/groups/2480461/items/UDYTZB76)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/UDYTZB76)
- Authors: [[Noam Shazeer]] [[Quoc Le]] [[Jeff Dean]] [[Geoffrey Hinton]]
- Topics: [[topic1]] [[topic2]]
- Venue: #ICLR
- Year: #2017
---
### Major Contributions
- Introduction of Sparseley-Gated [[MoE]]
---
### Secondary Contribution
- Focus in language modeling and machine translation
- It is our goal to train a trillion parameter model on a trillion-word corpus. We have not scaled our systems this far as of the writing of this paper, but it should be possible by adding more hardware. [[_2021_SwitchTransformers]]
- 
---
### Limitations/Future Work
- 
---
### Notes (Try to use backlinks)
- [[MoE]] was proposed in 1991 Jacbos et al (see also Jordan and Jacobs 1994)
- The mixture of experts is the whole model Eigen et al. (2013) introduce the idea of using multiple MoEs with their own gating networks as parts of a deep model.
	- They also allude in their conclusion to the potential to introduce sparsity, turning MoEs into a vehicle for computational computation.
---
