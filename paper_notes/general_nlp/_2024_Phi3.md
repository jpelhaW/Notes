### Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
---
- [Zotero Select Link](zotero://select/groups/2480461/items/7ASSCSKI)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/7ASSCSKI)
- Authors: [[Microsoft]]
- Topics: [[nlp_llm]] [[nlp_general]]
- Venue: #arxiv #microsoft 
- Year: #2023

---
### Major Contributions
- “We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone.” (Abdin et al., 2024, p. 1)
- “The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data.” (Abdin et al., 2024, p. 1)
---
### Secondary Contribution
- 
---
### Limitations/Future Work
- “Our training data of consists of heavily filtered web data (according to the “educational level”) from various open internet sources, as well as synthetic LLM-generated data.” (Abdin et al., 2024, p. 2) - no details on the training method, or the red team, 
- “Phi-3-mini was developed in accordance with Microsoft’s responsible AI principles. The overall approach consisted of safety alignment in post-training, red-teaming, automated testing and evaluations across dozens of RAI harm categories. Helpfulness and harmlessness preference datasets [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training. An independent red team at Microsoft iteratively examined phi-3-mini to further identify areas of improvement during the post-training process.” (Abdin et al., 2024, p. 5) This is as vague as it gets - what are those principles?
- “5 Weakness In terms of LLM capabilities, while phi-3-mini model achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much “factual knowledge”, which can be seen for example with low performance on TriviaQA.” (Abdin et al., 2024, p. 7)
---
### Notes (Try to use backlinks)
- “In our previous works on the phi models [GZA+23, LBE+23, JBA+23] it was shown that a combination of LLM-based filtering of web data, and LLM-created synthetic data, enable performance in smaller language models that were typically seen only in much larger models.” (Abdin et al., 2024, p. 1)
- “To best benefit the open source community, phi-3-mini is built upon a similar block structure as Llama-2 [TLI+23] and uses the same tokenizer with vocabulary size of 320641. This means that all packages developed for Llama-2 family of models can be directly adapted to phi-3-mini.” (Abdin et al., 2024, p. 2)
---
