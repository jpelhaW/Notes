### A Survey of Reasoning with Foundation Models
---
- [Zotero Select Link](zotero://select/groups/2480461/items/AUX7EAEV)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/AUX7EAEV)
- Authors: [[Jiankai Sun]] [[Zhenguo Li]]
- Topics: [[nlp_survey]] [[nlp_agent]]
- Venue: #arxiv
- Year: #2023

---
### Major Contributions
- “there remains a need for a systematic and comprehensive survey that specifically focuses on recent advancements in multimodal and interactive reasoning, which emulates human reasoning styles more closely.” (Sun et al., 2023, p. 5)
---
### Secondary Contribution
- 
---
### Limitations/Future Work
- 
---
### Notes (Try to use backlinks)
- “Foundation models typically consist of billions of parameters and undergo (pre-)training using self-supervised learning (Jain et al., 2023) on a broad dataset (Bommasani et al., 2021).” (Sun et al., 2023, p. 5)
- “reasoning with foundation models enables the application of prior knowledge and domain expertise. Logical rules can be derived from expert knowledge or formalized from existing ontologies or knowledge graphs.” (Sun et al., 2023, p. 6)
- “reasoning with foundation models can enhance the robustness and generalization capabilities. By incorporating the information contained in massive amounts of data, models can better handle situations facing limited data or encountering unseen scenarios during deployment” (Sun et al., 2023, p. 6)
- “In summary, we have conducted a survey of over 650 papers on foundation models, primarily focusing on research from the past two years. We discuss different tasks, approaches, techniques, and benchmarks used in these models” (Sun et al., 2023, p. 7)
- “Another noteworthy approach in this regard is Reasoning via Planning (RAP) (Hao et al., 2023a), which capitalizes on the language model’s dual role as both a world model and a reasoning agent. RAP incorporates a well-founded planning algorithm, specifically based on Monte Carlo Tree Search, to facilitate strategic exploration within the expansive realm of reasoning” (Sun et al., 2023, p. 40)
	- “The evaluations demonstrate RAP’s proficiency in addressing diverse reasoning challenges, effectively showcasing its versatility as a capable reasoning agent.” (Sun et al., 2023, p. 40)
- “Introspective reasoning, illustrated in Figure 12(a), relies solely on internal knowledge and reasoning to generate a static plan of tool use without interacting with the environment (Leake, 2012).” (Sun et al., 2023, p. 40)
	- Introspective reasoning may have limitations in dynamic and uncertain environments where external feedback and interaction with the environment are crucial for effective planning.
- “y actively engaging with the environment and utilizing feedback, extrospective reasoning offers a more flexible and responsive approach to generating plans, which is particularly suitable for complex and dynamic situations where the ability to adapt” (Sun et al., 2023, p. 41)
	- “Several related works in the field of extrospective reasoning with LLMs include Self-Ask (Press et al., 2023), ReAct (Yao et al., 2023c), ToolFormer (Schick et al., 2023), and LLM-Planner (Song et al., 2023a).” (Sun et al., 2023, p. 42)
- “In a similar vein, Du et al. (2023) propose a methodology that involves multiple instances of language models engaging in debates. Through iterative rounds of reasoning and response generation, these models collectively work towards reaching a common final answer. This approach has demonstrated significant improvements in mathematical and strategic reasoning across various tasks.” (Sun et al., 2023, p. 44)
- ![[2023_Reasoning_Foundation_Models_overview.png]]
- ![[2023_Reasoning_Foundation_Models_reason_diff.png]]
---
