### Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey
---
- [Zotero Select Link](zotero://select/groups/2480461/items/MMDPK3XF)
- [Zotero URI](https://www.zotero.org/groups/2480461/items/MMDPK3XF)
- Authors: [[Philipp Mondorf]]  [[Barbara Plank]] 
- Topics: [[nlp_reasoning]] [[nlp_survey]]
- Venue: #arxiv
- Year: #y2024

---
### Major Contributions
- “This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models’ reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses.” (Mondorf and Plank, 2024, p. 1)
- “Specifically, we address the following research questions: 
	- RQ1: How do current LLMs behave across diverse reasoning tasks? 
	- RQ2: What are the prevalent evaluation methods employed to assess the reasoning behavior of large language models?” (Mondorf and Plank, 2024, p. 2)
---
### Strengths
-  Survey focused on the reasoning and behaviours of models other than accuracy
- Good taxonomy
- “our review suggests that current models more closely resemble stochastic parrots (Bender et al., 2021) than systematic reasoners.” (Mondorf and Plank, 2024, p. 9) - expected
- “B Further Details on Evaluation Methods” (Mondorf and Plank, 2024, p. 25) has details on each evaluation method they talk
---
### Weakness
- They focus on core reasoning tasks (i.e., math, logical, causal). the integrated reasoning tasks (common sense, scientific, and socia) seem more interesting
- The rational for the paper selection is not disclosed or the papers themselves (as in a github)
- 
---
### Notes
- “Definition 2.1 (Reasoning). The process of drawing conclusions based on available information (usually a set of premises)” (Mondorf and Plank, 2024, p. 2)
- “Definition 2.2 (Reasoning Behavior). The system’s computed response to a reasoning task (the stimulus), particularly its actions, expressions and underlying mechanisms exhibited during the reasoning process” (Mondorf and Plank, 2024, p. 2)
- ![[2024_BeyondAccuracy_taxonomy.png]]
- “Conclusion-Based Evaluation In conclusion-based evaluation schemes, the emphasis is placed on the model’s final answer, rather than the process by which the model formulates its conclusion. This outcome-oriented approach, despite its limitation in overlooking the model’s rationales, can nonetheless provide insights into the model’s reasoning behavior.” (Mondorf and Plank, 2024, p. 8)
	- “an examination of the model’s output distribution may reveal inherent predispositions towards certain outcomes (Itzhak et al., 2023), or serve as an indicator of the model’s confidence towards specific conclusions (Frohberg & Binder, 2022).” (Mondorf and Plank, 2024, p. 8) 
- “Rationale-Based Evaluation In contrast to conclusion-based evaluation schemes, rationale-based evaluation methods focus on examining the reasoning trace generated by the model, typically assessing its logical validity and coherence.” (Mondorf and Plank, 2024, p. 8)
	- “Alternatively, interpretable quantitative metrics, such as ROSCOE (Golovneva et al., 2023) or RECEVAL (Prasad et al., 2023), may be utilized to evaluate the rationales’ semantic alignment with the reasoning task, their coherence, factual consistency, and logical validity” (Mondorf and Plank, 2024, p. 8)
- “Interactive Evaluation offers a framework to engage with LLMs during the evaluation.” (Mondorf and Plank, 2024, p. 9)
	- “adaptive evaluations dynamically select reasoning tasks based on the model’s responses, thus providing deeper insights into its capabilities and limitations beyond what static questionnaires can offer (Zhuang et al., 2023). Dialectic evaluation methods assess the model’s reasoning in dialogue form, for example by challenging the model’s conclusions (Wang et al., 2023), or engaging it in game-theoretical scenarios (Bertolazzi et al., 2023).” (Mondorf and Plank, 2024, p. 9)
- “Mechanistic Evaluation Mechanistic evaluations of LLMs delve into the underlying processes that drive the model’s responses, aiming to uncover the “how” and “why” within their reasoning process. By analyzing the internal mechanisms, such as attention patterns (Hou et al., 2023), activation flows (Dutta et al., 2024), and the functional attributes of individual layers (Pirozelli et al., 2023), deeper insights into the model’s operational logic can be gained.” (Mondorf and Plank, 2024, p. 9)
- “While human reasoning is not infallible (Johnson-Laird, 2010), the human capacity for robust reasoning and generalization from limited data is unmatched by current LLMs.” (Mondorf and Plank, 2024, p. 9)
- “The findings reveal that, similar to humans, LLMs are prone to logical fallacies and cognitive biases such as ordering effects. This susceptibility persists across model sizes, though larger models tend to engage in more deliberate reasoning, showing a reduced sensitivity towards these errors. A notable finding indicates that LLMs, unlike humans, rarely produce the “nothing follows” response, even when it represents the accurate deduction. I” (Mondorf and Plank, 2024, p. 24)
	- “Comprehensive manual evaluations of the models’ rationales reveal that LLMs utilize inferential strategies similar to those employed by human reasoners. Their evaluations further underline difficulties of LLMs with logical negations and their vulnerability to logical fallacies commonly observed in human reasoning.” (Mondorf and Plank, 2024, p. 24)
- ---
### Questions
- 
---
